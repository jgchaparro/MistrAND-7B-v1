{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference on MistrAND 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations \n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U torch\n",
    "!pip install -q -U huggingface_hub\n",
    "!pip install -q -U ipywidgets\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U roman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load main model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import accelerate\n",
    "\n",
    "# Others\n",
    "import re\n",
    "import itertools\n",
    "import roman\n",
    "import sys\n",
    "running_on_colab = 'google.colab' in sys.modules\n",
    "\n",
    "# Hugging Face login\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into HuggingFace\n",
    "# Alternatively, use !huggingface-cli login --token \n",
    "if running_on_colab:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, load the QLoRA adapter from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = PeftModel.from_pretrained(base_model, \"jgchaparro/MistrAND-7B-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Andalusian Spanish converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, import the converter to transliterate texts from Standard Spanish to Andalusian Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running locally, define the class\n",
    "if running_on_colab:\n",
    "    class AndalusianConverter:\n",
    "        def __init__(self, \n",
    "                    rotacism: bool = False):\n",
    "            \"\"\"\n",
    "            Initializes the class with the necessary parameters.\n",
    "\n",
    "            Parameters:\n",
    "            - rotacism (bool): whether to apply rotacism (transformation of /l/ into /r/ before consonants).\n",
    "                            Default is False.\n",
    "            \"\"\"\n",
    "            ### Sets ###\n",
    "            self.NON_ACCENTED_VOWELS = 'aeiouAEIOUÜ'\n",
    "            self.ACCENTED_VOWELS = 'áéíóúÁÉÍÓÚ'\n",
    "            self.VOWELS = self.NON_ACCENTED_VOWELS + self.ACCENTED_VOWELS\n",
    "\n",
    "            self.ORIGINAL_CONSONANTS = 'bcdfghjklmnñpqrstvwxyzBCDFGHJKLMNÑPQRSTVWXYZ'\n",
    "            self.NEW_CONSONANTS = 'ʌъƨьɿзбɅЪƧЬႨЗГБ'\n",
    "            self.CONSONANTS = self.ORIGINAL_CONSONANTS + self.NEW_CONSONANTS\n",
    "\n",
    "            self.LETTERS = self.VOWELS + self.CONSONANTS\n",
    "            self.STOPCHARS = '.,:;?!¡¿ $\\(\\)\\[\\]\\{\\}«»\\'\\\"'\n",
    "\n",
    "            ### Conversions ###\n",
    "            # Accents\n",
    "            self.TO_ACCENTED_VOWEL = {\n",
    "                'a': 'á',\n",
    "                'e': 'é',\n",
    "                'i': 'í',\n",
    "                'o': 'ó',\n",
    "                'u': 'ú',\n",
    "                'A': 'Á',\n",
    "                'E': 'É',\n",
    "                'I': 'Í',\n",
    "                'O': 'Ó',\n",
    "                'U': 'Ú',\n",
    "            }\n",
    "\n",
    "            self.TO_NON_ACCENTED_VOWEL = {v: k for k, v in self.TO_ACCENTED_VOWEL.items()}\n",
    "\n",
    "            # Uppercasing and lowercasing\n",
    "            self.UPPERCASE = {\n",
    "                'a' : 'A',\n",
    "                'б' : 'Б',\n",
    "                'c' : 'C',\n",
    "                'd' : 'D',\n",
    "                'e' : 'E',\n",
    "                'f' : 'F',\n",
    "                'ƨ' : 'Ƨ',\n",
    "                'ь' : 'Ь',\n",
    "                'i' : 'I',\n",
    "                'l' : 'L',\n",
    "                'm' : 'M',\n",
    "                'n' : 'N',\n",
    "                'o' : 'O',\n",
    "                'p' : 'P',\n",
    "                'r' : 'Γ',\n",
    "                'ъ' : 'Ъ',\n",
    "                'ʌ' : 'Ʌ',\n",
    "                'u' : 'U',\n",
    "                'w' : 'W',\n",
    "                'y' : 'Y',\n",
    "                'ч' : 'Ч',\n",
    "                'ɿ' : 'Ⴈ'\n",
    "            }\n",
    "\n",
    "            self.LOWERCASE = {v: k for k, v in self.UPPERCASE.items()}\n",
    "\n",
    "            # Measurement abbreviations\n",
    "            self.MEASUREMENT_ABBREVIATIONS = {\n",
    "                # Time\n",
    "                's' : 'ъ',\n",
    "                'm' : 'm',\n",
    "                'h' : 'o',\n",
    "                # Distance\n",
    "                'mm' : 'mm',\n",
    "                'cm' : 'ɿm',\n",
    "                'dm' : 'dm',\n",
    "                # 'm' : 'm', # Duplicated with minutes\n",
    "                'Dm' : 'Dm',\n",
    "                'hm' : 'em',\n",
    "                'km' : 'cm',\n",
    "                # Litres\n",
    "                'ml' : 'll',\n",
    "                'cl' : 'ɿl',\n",
    "                'dl' : 'dl',\n",
    "                'l' : 'l',\n",
    "                'Dl' : 'Dl',\n",
    "                'hl' : 'el',\n",
    "                'kl' : 'cl',\n",
    "            }\n",
    "\n",
    "            ### Flags ###\n",
    "            self.rotacism = rotacism\n",
    "\n",
    "        ### Methods ###\n",
    "            \n",
    "        def show_text(self):\n",
    "            \"\"\"\n",
    "            Formats the text for ease of use\n",
    "            \"\"\"\n",
    "\n",
    "            lines = self.text.split(', ')\n",
    "            for line in lines:\n",
    "                print(line.strip())\n",
    "\n",
    "\n",
    "        ### Step 0: preprocessing ###\n",
    "            \n",
    "        def clean_text(self):\n",
    "            \"\"\"\n",
    "            Applies basic preliminary cleanings to the text. Including the following:\n",
    "            - Remove phantom separation occurring in Wikipedia articles when two references are next to each other.\n",
    "            - Remove numbers after a word.\n",
    "            \"\"\"\n",
    "\n",
    "            # Remove phantom spaces\n",
    "            self.text = self.text.replace('​', '')\n",
    "\n",
    "            # Remove numbers after a word\n",
    "            pattern = r'([a-zA-Z]+)\\d+'\n",
    "            output = r'\\1'\n",
    "            self.text = re.sub(pattern, output, self.text)\n",
    "        \n",
    "\n",
    "        ### Step 1: direct replacements ###\n",
    "                    \n",
    "        def apply_direct_replacements(self):\n",
    "            \"\"\"\n",
    "            Modifies directly certain expressions.\n",
    "            \"\"\"\n",
    "\n",
    "            direct_replacements = {\n",
    "                # Standard Spanish : Andalusian Spanish\n",
    "                f'([{self.STOPCHARS}])para([{self.STOPCHARS}])' : r'\\1pa\\2',\n",
    "                f'([{self.STOPCHARS}])Para([{self.STOPCHARS}])' : r'\\1Pa\\2',\n",
    "                f'([{self.STOPCHARS}])muy([{self.STOPCHARS}])' : r'\\1mu\\2',\n",
    "                f'([{self.STOPCHARS}])Muy([{self.STOPCHARS}])' : r'\\1Mu\\2',\n",
    "                f'([{self.STOPCHARS}])todo([{self.STOPCHARS}])' : r'\\1ʌo\\2',\n",
    "                f'([{self.STOPCHARS}])Todo([{self.STOPCHARS}])' : r'\\1Ʌo\\2',\n",
    "                f'([{self.STOPCHARS}])toda([{self.STOPCHARS}])' : r'\\1ʌoa\\2',\n",
    "                f'([{self.STOPCHARS}])Toda([{self.STOPCHARS}])' : r'\\1Ʌoa\\2',\n",
    "                f'([{self.STOPCHARS}])todos([{self.STOPCHARS}])' : r'\\1ʌoь\\2',\n",
    "                f'([{self.STOPCHARS}])Todos([{self.STOPCHARS}])' : r'\\1Ʌoь\\2',\n",
    "                f'([{self.STOPCHARS}])todas([{self.STOPCHARS}])' : r'\\1ʌoaь\\2',\n",
    "                f'([{self.STOPCHARS}])Todas([{self.STOPCHARS}])' : r'\\1Ʌoaь\\2',\n",
    "                f'([{self.STOPCHARS}])pues([{self.STOPCHARS}])' : r'\\1poь\\2',\n",
    "                f'([{self.STOPCHARS}])Pues([{self.STOPCHARS}])' : r'\\1Poь\\2',\n",
    "                f'([{self.STOPCHARS}])etc([{self.STOPCHARS}])' : r'\\1eьɿ\\2',\n",
    "            }\n",
    "\n",
    "            # Loop through the dictionary\n",
    "            for key, value in direct_replacements.items():\n",
    "                self.text = re.sub(key, value, self.text)\n",
    "\n",
    "\n",
    "        def replace_roman_numerals(self):\n",
    "            \"\"\"\n",
    "            Replaces roman numerals with arabic numerals. \n",
    "            It is assumed that all instances of roman numerals are in the form \"siglo \" + roman numeral\n",
    "            or \"siglos \" + roman numeral.\n",
    "            \"\"\"\n",
    "\n",
    "            # Find instances of \"siglo \" + roman numerals\n",
    "            # Replace by patterns, Longer patterns first\n",
    "            roman_patterns = [\n",
    "                (rf'siglos? [ivxlcdmIVXLCDM]+ y ([ivxlcdmIVXLCDM]+)[{self.STOPCHARS}]', f'siglos? [ivxlcdmIVXLCDM]+ y [ivxlcdmIVXLCDM]+[{self.STOPCHARS}]'),\n",
    "                (rf'siglos? ([ivxlcdmIVXLCDM]+)[{self.STOPCHARS}]', rf'siglos? [ivxlcdmIVXLCDM]+[{self.STOPCHARS}]')\n",
    "            ]\n",
    "\n",
    "            # If there are any, extract the roman numerals and convert them to arabic\n",
    "            siglo_relaces = {}\n",
    "            for pattern, clean_pattern in roman_patterns:    \n",
    "                siglos = re.findall(clean_pattern, self.text)\n",
    "                for siglo in siglos:\n",
    "                    # Extract the roman numerals\n",
    "                    has_pattern = re.match(pattern, siglo) is not None\n",
    "                    if has_pattern:\n",
    "                        roman_numeral = re.match(pattern, siglo).group(1)\n",
    "                        arabic_numeral = roman.fromRoman(roman_numeral.upper())\n",
    "                        # Replace the roman numerals with the arabic numerals\n",
    "                        siglo_relaces[siglo] = siglo.replace(roman_numeral, str(arabic_numeral))\n",
    "\n",
    "            # Loop through the dictionary\n",
    "            for key, value in siglo_relaces.items():\n",
    "                self.text = self.text.replace(key, value)\n",
    "\n",
    "        \n",
    "        ### Step 2: contextual replacements ###\n",
    "                \n",
    "        def apply_contextual_conversions(self):\n",
    "            \"\"\"\n",
    "            Applies conself.textual conversions to the self.text.\n",
    "\n",
    "            Some letters depend on the conself.text to be transformed. This implies that order matters.\n",
    "\n",
    "            * x\n",
    "                * If it is at the beginning of a word, it is transformed into \"ъ\".\n",
    "                * If it is at the end of a word, it is transformed into \"ь\".\n",
    "                * If it is enclosed between two vowels, it is transformed into \"ьъ\".\n",
    "                * Otherwise, it is transformed into \"ь\".\n",
    "\n",
    "            * g\n",
    "                * If followed by \"e\" or \"i\", it is transformed into \"ь\".\n",
    "                * Otherwise, it is transformed into \"ƨ\".\n",
    "\n",
    "            * c\n",
    "                * If followed by \"e\" or \"i\", it is transformed into \"ɿ\".\n",
    "                * Otherwise, it is transformed into \"c\".\n",
    "            \"\"\"\n",
    "\n",
    "            CONTEXTUAL_CONVERSIONS = {\n",
    "                ### g\n",
    "                # Lowercase\n",
    "                'g([eiéí])' : r'ь\\1',\n",
    "                'gu([eiéí])' : r'ƨ\\1',\n",
    "                'gü' : 'w',\n",
    "                'g([aoáó])' : r'ƨ\\1',\n",
    "                'gu([aoáó])' : r'w\\1',\n",
    "                # Uppercase\n",
    "                'G([eiEIéíÉÍ])' : r'Ь\\1',\n",
    "                'Gu([eiEIéíÉÍ])' : r'Ƨ\\1',\n",
    "                'G[üÜ]' : 'W',\n",
    "                'G([aoAOáóÁÓ])' : r'Ƨ\\1',\n",
    "                'Gu([aoAOáóÁÓ])' : r'W\\1',\n",
    "\n",
    "                ### c\n",
    "                # Lowercase\n",
    "                'c([eiéíEIÉÍ])' : r'ɿ\\1',\n",
    "                # Uppercase\n",
    "                'C([eiéíEIÉÍ])' : r'Ⴈ\\1',\n",
    "\n",
    "                ### x\n",
    "                # Lowercase\n",
    "                f'([{self.VOWELS}])x([{self.VOWELS}])' : r'\\1ьъ\\2',\n",
    "                f'x([{self.CONSONANTS}])' : r'ь\\1',\n",
    "                # f'x([{stopchars}])' : r'ь\\1', # Special case to be handled in word-end transformations\n",
    "                f'([{self.VOWELS}])X([{self.VOWELS}])' : r'\\1ЬЪ\\2',\n",
    "                f'X([{self.CONSONANTS}])' : r'Ь\\1',\n",
    "                # f'X([{stopchars}])' : r'Ь\\1', # Special case to be handled in word-end transformations\n",
    "\n",
    "                ### y\n",
    "                # Lowercase\n",
    "                f'([{self.VOWELS}{self.STOPCHARS}])y([{self.CONSONANTS}{self.STOPCHARS}])' : r'\\1i\\2',\n",
    "                # Uppercase\n",
    "                f'([{self.VOWELS}{self.STOPCHARS}])Y([{self.CONSONANTS}{self.STOPCHARS}])' : r'\\1I\\2',\n",
    "            }\n",
    "\n",
    "            # Loop through the dictionary\n",
    "            for key, value in CONTEXTUAL_CONVERSIONS.items():\n",
    "                self.text = re.sub(key, value, self.text)\n",
    "\n",
    "        \n",
    "        ### Step 3: direct conversions ###\n",
    "        \n",
    "        def apply_direct_conversions(self):\n",
    "            \"\"\"\n",
    "            Applies direct conversions to the self.text.\n",
    "\n",
    "            The following letters can always be converted to the same letter in Andalusian, **regardless of the conself.text**:\n",
    "\n",
    "            | Letter | Translit. |\n",
    "            |--------|-----------|\n",
    "            | b      | б         |\n",
    "            | v      | б         |\n",
    "            | k      | c         |\n",
    "            | qu     | c         |\n",
    "            | t      | ʌ         |\n",
    "            | ll     | y         |\n",
    "            | s      | ъ         |\n",
    "            | j      | ь         |\n",
    "            | z      | ɿ         |\n",
    "            | ch     | ч         |\n",
    "            | h      |           |\n",
    "            \"\"\"\n",
    "\n",
    "            # Dictionary of direct conversions\n",
    "            DIRECT_CONVERSIONS = {\n",
    "            'b' : 'б',\n",
    "            'B' : 'Б',\n",
    "            'v' : 'б',\n",
    "            'V' : 'Б',\n",
    "            'g' : 'ƨ',\n",
    "            'G' : 'Ƨ',\n",
    "            'k' : 'c',\n",
    "            'K' : 'C',\n",
    "            'qu' : 'c',\n",
    "            'Qu' : 'C',\n",
    "            't' : 'ʌ',\n",
    "            'T' : 'Ʌ',\n",
    "            'll' : 'y',\n",
    "            'Ll' : 'Y',\n",
    "            's' : 'ъ',\n",
    "            'S' : 'Ъ',\n",
    "            'j' : 'ь',\n",
    "            'J' : 'Ь',\n",
    "            'z' : 'ɿ',\n",
    "            'Z' : 'Ⴈ',\n",
    "            'ch' : 'ч',\n",
    "            'Ch' : 'Ч',\n",
    "            'h' : '',\n",
    "            'R' : 'Γ'\n",
    "            }\n",
    "\n",
    "            # Create a regular expression that matches any key in the dictionary\n",
    "            regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, DIRECT_CONVERSIONS.keys())))\n",
    "\n",
    "            # Replace the matches with the values in the dictionary\n",
    "            self.text = regex.sub(lambda x: DIRECT_CONVERSIONS[x.group(0)], self.text)\n",
    "        \n",
    "\n",
    "        def capitalize_post_h_consonants(self):\n",
    "            \"\"\"\n",
    "            Capitalizes the second letter of all instances of \"H\" followed by a consonant.\n",
    "            \"\"\"\n",
    "\n",
    "            # Find all H instances\n",
    "            capital_h = re.findall('H.', self.text)\n",
    "\n",
    "            # Loop through the list creating replaces where the H is replaced by the second letter capitalized\n",
    "            replaces = {combination : combination.upper()[1] for combination in capital_h}\n",
    "            for key, value in replaces.items():\n",
    "                self.text = self.text.replace(key, value)\n",
    "\n",
    "        ### Step 4: word-end transformations ###\n",
    "                \n",
    "        # Auxiliar methods\n",
    "        def detach_stopchars(self,\n",
    "                            word: str):\n",
    "            \"\"\"Separates the stopchars from the word and returns both separately\"\"\"\n",
    "\n",
    "            # If there are no stopchars, return the word as is and None\n",
    "            if not re.search(f'[{self.STOPCHARS}]$', word):\n",
    "                return word, None\n",
    "            \n",
    "            # Otherwise, detach stopchars from the word if they are present\n",
    "            else:\n",
    "                word_stopchars = re.findall(f'([{self.STOPCHARS}]+)$', word)\n",
    "                assert len(word_stopchars) == 1, f'Unexpected number of stopchars found: {word_stopchars} for word {word}'\n",
    "                word_stopchars = word_stopchars[-1]\n",
    "                word = re.sub(f'([{self.STOPCHARS}]+)$', '', word)\n",
    "\n",
    "                return word, word_stopchars\n",
    "            \n",
    "        \n",
    "        def remove_final_consontants(self,\n",
    "                                    word: str) -> str:\n",
    "            \"\"\"Removes final consonants from a word if they are present.\"\"\"\n",
    "\n",
    "            # Remove consonants at the end of the word if they are present\n",
    "            end_consonants = re.findall(f'([{self.CONSONANTS}]+)$', word)\n",
    "            if end_consonants:\n",
    "                # Remove the consonants\n",
    "                word = re.sub(f'([{self.CONSONANTS}]+)$', '', word)\n",
    "            \n",
    "            return word\n",
    "        \n",
    "\n",
    "        def add_accent_mark(self,\n",
    "                            word: str) -> str:\n",
    "            \"\"\"\n",
    "            Adds an accent mark to the last vowel of a word if necessary.\n",
    "            \"\"\"\n",
    "\n",
    "            # Remove consonants at the end of the word if they are present\n",
    "            end_consonants = re.findall(f'([{self.CONSONANTS}]+)$', word)\n",
    "            if end_consonants:\n",
    "                # Remove the consonants\n",
    "                word = re.sub(f'([{self.CONSONANTS}]+)$', '', word)    \n",
    "\n",
    "                # Add an accent mark to the last vowel\n",
    "                try:\n",
    "                    last_letter = word[-1]    \n",
    "                # In some cases, i.e., abbreviatures (cm), the whole word can be deleted.\n",
    "                # If it happens, return the raw word\n",
    "                except IndexError: \n",
    "                    return word\n",
    "                \n",
    "                # Check if the remaining last character is a vowel\n",
    "                # If not, it might be a model name, a unit of measure or a special character like \"'\"\n",
    "                # In this case, return the word as is.\n",
    "                last_char_check = re.search(f'[{self.VOWELS}]$', word) is not None\n",
    "                if not last_char_check:\n",
    "                    return word\n",
    "\n",
    "                assert last_letter in self.VOWELS, f'Unexpected last letter of word: {word[-1]} for word {word}'\n",
    "                word_root = word[:-1]\n",
    "                marked_letter = self.TO_ACCENTED_VOWEL[last_letter]\n",
    "                word = word_root + marked_letter\n",
    "\n",
    "            return word\n",
    "        \n",
    "\n",
    "        def remove_accent_mark(self,\n",
    "                            word: str) -> str:\n",
    "            \"\"\"\n",
    "            Removes an accent mark from the last valid vowel of a word if necessary.\n",
    "            \"\"\"\n",
    "\n",
    "            # Find all accented vowels\n",
    "            aux_accented = re.findall(f\"[{self.ACCENTED_VOWELS}]\", word)\n",
    "\n",
    "            # If there are no accented vowels, return the word as is\n",
    "            if not aux_accented:\n",
    "                return word\n",
    "            \n",
    "            # If there are accented vowels, replace them with their non-accented counterparts\n",
    "            for accented_vowel in set(aux_accented):\n",
    "                word = word.replace(accented_vowel, self.TO_NON_ACCENTED_VOWEL[accented_vowel])\n",
    "\n",
    "            return word\n",
    "\n",
    "        \n",
    "        def handle_d_endings(self,\n",
    "                            word: str) -> str:\n",
    "            \"\"\"\n",
    "            Handles the -d- ending in words.\n",
    "            \"\"\"\n",
    "\n",
    "            # If the word is stressed in the third-to-last syllable,\n",
    "            # no changes are necessary\n",
    "            aux_mask = (\n",
    "                re.search(rf'[{self.NON_ACCENTED_VOWELS}]d[oa]ь?$', word) is not None and\n",
    "                re.search(f'[{self.ACCENTED_VOWELS}]', word) is not None\n",
    "            )\n",
    "            if aux_mask: \n",
    "                return word\n",
    "            \n",
    "            # In other cases, drop intervocalic -d- in the last syllable and manage accents\n",
    "            pattern = rf'([aeiuáéíúAEIUÁÉÍÚ])d([oóa])(ь)?$'\n",
    "            output = rf'\\1\\2\\3'\n",
    "            word = re.sub(pattern, output, word)\n",
    "\n",
    "            # Handle accents\n",
    "            aux_accent_dict  = {\n",
    "                'ao(ь)?$' : r'áo\\1',\n",
    "                'aa(ь)?$' : r'á\\1',\n",
    "                'i([ao])(ь)?$' : r'í\\1\\2',\n",
    "                'u([ao])(ь)?$' : r'ú\\1\\2'\n",
    "            }\n",
    "            for pattern, output in aux_accent_dict.items():\n",
    "                if re.search(pattern, word) is not None:\n",
    "                    word = re.sub(pattern, output, word)\n",
    "                    break\n",
    "\n",
    "            return word \n",
    "        \n",
    "\n",
    "        def perform_word_end_transformations(self,\n",
    "                                            word: str) -> str:\n",
    "            \"\"\"\n",
    "            Wrapper function to perform changes at the end of words,\n",
    "            managing also accent marks.\n",
    "            \"\"\"\n",
    "\n",
    "            # Determine sets\n",
    "            second_to_last_accented_consonants = 'ʌdpбcƨfrl'\n",
    "            last_accented_consonants = 'ɿm'\n",
    "            word_end_transformations = {\n",
    "                'ъ' : {'pattern' : rf'ъ([{self.STOPCHARS}]+)?$', 'output' : r'ь\\1'},\n",
    "                'ɿ' : {'pattern' : rf'ɿ([{self.STOPCHARS}]+)?$', 'output' : r'ь\\1'},\n",
    "                'm' : {'pattern' : rf'm([{self.STOPCHARS}]+)?$', 'output' : r'n\\1'},\n",
    "            }\n",
    "\n",
    "            # Set words that remain unchanged\n",
    "            INMUTABLE_EXCEPTIONS = ['el', 'del', 'al', 'por', 'eьɿ']\n",
    "            INMUTABLE_EXCEPTIONS.extend([word.capitalize() for word in INMUTABLE_EXCEPTIONS])\n",
    "\n",
    "            # Detect last letter to perform the appropriate transformation\n",
    "            original_word = word\n",
    "            word, word_stopchars = self.detach_stopchars(word)\n",
    "        \n",
    "            # Some stopchars combinations (e.g. (...)) or inmutable exceptions with stopwords can get here \n",
    "            # If it happens, return the word as is\n",
    "            if word == '' or word in INMUTABLE_EXCEPTIONS:\n",
    "                return original_word\n",
    "            # Otherwise, get the last letter\n",
    "            else:\n",
    "                last_letter = word[-1]\n",
    "\n",
    "            ### Handle special cases ###\n",
    "            \n",
    "            # Skip inmutable words or words with just one character\n",
    "            if word in INMUTABLE_EXCEPTIONS or len(word) == 1:\n",
    "                if word_stopchars: # Attach stopchars if they are present\n",
    "                    word += word_stopchars            \n",
    "                return word\n",
    "            \n",
    "            # Units of measure should remain unchanged\n",
    "            if word in self.MEASUREMENT_ABBREVIATIONS.values():\n",
    "                if word_stopchars: # Attach stopchars if they are present\n",
    "                    word += word_stopchars\n",
    "                return word\n",
    "\n",
    "            # Check if the word has an accent mark\n",
    "            has_accent = re.search(f'[{self.ACCENTED_VOWELS}]', word) is not None\n",
    "\n",
    "            ### Handle last consonant cases ###\n",
    "\n",
    "            # Handle special cases first\n",
    "            # -x\n",
    "            if last_letter in 'xX':\n",
    "                if has_accent:\n",
    "                    word = self.remove_accent_mark(word)\n",
    "                else:\n",
    "                    word = self.add_accent_mark(word)\n",
    "                    \n",
    "                word = re.sub('x$', 'ь', word)\n",
    "                word = re.sub('X$', 'Ь', word)\n",
    "\n",
    "            # -pъ\n",
    "            elif word[-2:].lower() == 'pъ':\n",
    "                if has_accent:\n",
    "                    word = self.remove_accent_mark(word)\n",
    "                else:\n",
    "                    word = self.add_accent_mark(word)\n",
    "                    \n",
    "                word = re.sub('pъ$', 'ь', word)\n",
    "                word = re.sub('PЪ$', 'Ь', word)\n",
    "\n",
    "            # Words with second to last accented consonants\n",
    "            elif last_letter in second_to_last_accented_consonants:\n",
    "                if has_accent:\n",
    "                    word = self.remove_accent_mark(word)\n",
    "                else:\n",
    "                    word = self.add_accent_mark(word)\n",
    "                    \n",
    "                word = self.remove_final_consontants(word)\n",
    "            \n",
    "            # Words with last accented consonants\n",
    "            elif last_letter in last_accented_consonants:\n",
    "                if has_accent:\n",
    "                    word = self.remove_accent_mark(word)\n",
    "                    pattern = word_end_transformations[last_letter]['pattern']\n",
    "                    output = word_end_transformations[last_letter]['output']\n",
    "                    word.replace(pattern, output)\n",
    "                else:\n",
    "                    word = self.add_accent_mark(word)\n",
    "                    transformed_ending = word_end_transformations[last_letter]['output'].replace(r'\\1', '')\n",
    "                    word += transformed_ending\n",
    "\n",
    "            # Special case for 'ъ': the accent rules do not change\n",
    "            elif last_letter == 'ъ':\n",
    "                pattern = word_end_transformations[last_letter]['pattern']\n",
    "                output = word_end_transformations[last_letter]['output']\n",
    "                word = re.sub(pattern, output, word)\n",
    "\n",
    "            # Handle -d- endings\n",
    "            if re.search(f'[{self.VOWELS}]d[{self.VOWELS}{self.ACCENTED_VOWELS}]ь?', word) is not None:\n",
    "                word = self.handle_d_endings(word)\n",
    "\n",
    "            # Attach stopchars from the word if they are present\n",
    "            if word_stopchars:\n",
    "                word += word_stopchars\n",
    "            \n",
    "            return word\n",
    "        \n",
    "        def word_end_transformation_wrapper(self):\n",
    "            \"\"\"\n",
    "            Wrapper function to perform word-end transformations.\n",
    "            \"\"\"\n",
    "\n",
    "            # Split the text into words, apply the transformations and join them back.\n",
    "            lines = self.text.strip().split('\\n')\n",
    "            processed_text = ''\n",
    "            for line in lines:\n",
    "                # Split, transform and join the words\n",
    "                words = line.split(' ')\n",
    "                words = [self.perform_word_end_transformations(word) for word in words if word != '' and word not in self.STOPCHARS]    \n",
    "                words = ' '.join(words)\n",
    "\n",
    "                # Add it back to the text\n",
    "                processed_text += words + '\\n'\n",
    "\n",
    "                # Set it back to the text\n",
    "                self.text = processed_text\n",
    "\n",
    "            # Remove the last newline character\n",
    "            self.text = self.text[:-1]\n",
    "\n",
    "\n",
    "        ### Step 5: word initial transformations ###\n",
    "                \n",
    "        def apply_initial_transformations(self):\n",
    "            \"\"\"\n",
    "            Applies initial transformations to the text.\n",
    "            \"\"\"\n",
    "\n",
    "            # Dictionary of initial transformations\n",
    "            WORD_INITIAL_TRANSFORMATIONS = {\n",
    "                f'([^{self.LETTERS}])pъ([{self.LETTERS}])' : r'\\1ъ\\2',\n",
    "                f'([^{self.LETTERS}])Pъ([{self.LETTERS}])' : r'\\1Ъ\\2',\n",
    "                f'([^{self.LETTERS}])mn([{self.LETTERS}])' : r'\\1n\\2',\n",
    "                f'([^{self.LETTERS}])Mn([{self.LETTERS}])' : r'\\1N\\2',\n",
    "                f'([^{self.LETTERS}])u([eiéí])([{self.LETTERS}])' : r'\\1w\\2\\3',\n",
    "                f'([^{self.LETTERS}])U([eiéíEIÉÍ])([{self.LETTERS}])' : r'\\1W\\2\\3',\n",
    "                \n",
    "            }\n",
    "\n",
    "            # Loop through the dictionary\n",
    "            for key, value in WORD_INITIAL_TRANSFORMATIONS.items():\n",
    "                self.text = re.sub(key, value, self.text)\n",
    "        \n",
    "\n",
    "        ### Step 6: internal word transformations ###\n",
    "                \n",
    "        def apply_r_transformations(self):\n",
    "            \"\"\"\n",
    "            Substitutes 'r' with 'n' or 'l' depending on the context.\n",
    "            \"\"\"\n",
    "\n",
    "            # 'r'-substitutions\n",
    "            R_SUBSTITUTION = {'pattern' : r'r([ln])', 'output' : r'\\1\\1'}\n",
    "            self.text = re.sub(R_SUBSTITUTION['pattern'], R_SUBSTITUTION['output'], self.text)\n",
    "\n",
    "\n",
    "        def simplify_consonant_clusters(self):\n",
    "            \"\"\"\n",
    "            Simplifies three-letter consonant clusters.\n",
    "            \"\"\"\n",
    "\n",
    "            CONSONANT_CLUSTERS_REPLACES = {\n",
    "                f'([{self.VOWELS}])[бn]ъ([{self.CONSONANTS}])' : r'\\1ь\\2'\n",
    "                }\n",
    "\n",
    "            # Loop through the dictionary\n",
    "            for key, value in CONSONANT_CLUSTERS_REPLACES.items():\n",
    "                self.text = re.sub(key, value, self.text)\n",
    "\n",
    "        \n",
    "        def apply_reductor_collisions(self):\n",
    "            \"\"\"\n",
    "            Reductors are a set of consonants that, when grouped together, the first one is reduced to 'ь'. \n",
    "            \"\"\"\n",
    "\n",
    "            # Reductor colisions\n",
    "            REDUCTORS = 'cƨʌdьъɿpбf'\n",
    "            REDUCTOR_COMBINATIONS = [''.join(pair) for pair in itertools.permutations(REDUCTORS, 2)]\n",
    "            REDUCTOR_REPLACES = {pair : 'ь' + pair[1] for pair in REDUCTOR_COMBINATIONS}\n",
    "\n",
    "            # Loop through the dictionary\n",
    "            for key, value in REDUCTOR_REPLACES.items():\n",
    "                self.text = re.sub(key, value, self.text)\n",
    "\n",
    "            # 'n' acts as a reductor when placed after another reductor, but it is immune to this process\n",
    "            self.text = re.sub(f'[{REDUCTORS}]n', r'ьn', self.text)\n",
    "\n",
    "        \n",
    "        def apply_other_simplifications(self):\n",
    "            \"\"\"\n",
    "            Applies other specific simpifications changes to the text.\n",
    "            \"\"\"\n",
    "\n",
    "            OTHER_SIMPLIFICATIONS = {\n",
    "                'nб' : 'mб',\n",
    "                'ee' : 'e',\n",
    "                'nm' : 'mm',\n",
    "            }\n",
    "\n",
    "            for key, value in OTHER_SIMPLIFICATIONS.items():\n",
    "                self.text = self.text.replace(key, value)\n",
    "\n",
    "        \n",
    "        ### Step 7: weak particles assimilation ###\n",
    "                \n",
    "        def apply_weak_particles_assimilation(self):\n",
    "            \"\"\"\n",
    "            Several common use particles loose their vowels and are attached to nearby words, forming contractions.\n",
    "            \"\"\"\n",
    "\n",
    "            assimilations_dict = {\n",
    "                f'([{self.STOPCHARS}])([dʌъlm])e ([{self.VOWELS}])' : r\"\\1\\2'\\3\",\n",
    "                f'([{self.STOPCHARS}])([DɅЪLM])e ([{self.VOWELS}])' : r\"\\1\\2'\\3\",\n",
    "                f'([{self.STOPCHARS}])c[eé] ([{self.VOWELS}])' : r\"\\1c'\\2\",\n",
    "                f'([{self.STOPCHARS}])C[eé] ([{self.VOWELS}])' : r\"\\1C'\\2\",\n",
    "                f'([{self.STOPCHARS}])la a' : r\"\\1l'a\",\n",
    "                f'([{self.STOPCHARS}])La a' : r\"\\1L'a\",\n",
    "                f'([{self.STOPCHARS}])lo o' : r\"\\1l'o\",\n",
    "                f'([{self.STOPCHARS}])Lo o' : r\"\\1L'o\",\n",
    "                f'([aeoáéó]) e([ln])([{self.STOPCHARS}])' : r\"\\1'\\2\\3\", # NOTE: review\n",
    "                f'([{self.VOWELS}]) e([{self.CONSONANTS}]{2,})' : r\"\\1'\\2\", # NOTE: review\n",
    "                f'o ([oóOÓ])' : r\"'\\1\",\n",
    "            }\n",
    "\n",
    "            for key, value in assimilations_dict.items():\n",
    "                self.text = re.sub(key, value, self.text)\n",
    "\n",
    "        \n",
    "        ### Step 8: rotacism ###\n",
    "                \n",
    "        def apply_rotacism(self):\n",
    "            \"\"\"\n",
    "            'l' before 'r' can convert to 'r' before consonants in some speakers.\n",
    "            This function applies rotacism to the text.\n",
    "            \"\"\"\n",
    "\n",
    "            # Lowercase\n",
    "            pattern = f'l( ?)([{self.CONSONANTS}])'\n",
    "            output = r'r\\1\\2'\n",
    "            self.text = re.sub(pattern, output, self.text)\n",
    "\n",
    "            # Uppercase\n",
    "            pattern = f'L( ?)([{self.CONSONANTS}])'\n",
    "            output = r'Γ\\1'\n",
    "            self.text = re.sub(pattern, output, self.text)\n",
    "\n",
    "        ### Step 9: space-separated 'r' assimilation\n",
    "            \n",
    "        def apply_space_separated_r_assimilation(self):\n",
    "            \"\"\"\n",
    "            If a word preserves a final 'r' and the next word starts with 'l' or 'n', the 'r' is assimilated.\n",
    "            \"\"\"\n",
    "\n",
    "            # Apply space-separated \"r\" assimilation\n",
    "            SPACE_SEPARATED_R_PATTERN = 'r ([lnLN])'\n",
    "            output = r'\\1 \\1'\n",
    "            self.text = re.sub(SPACE_SEPARATED_R_PATTERN, output, self.text)\n",
    "\n",
    "        ### Main method ###\n",
    "        def convert(self, text):\n",
    "            \"\"\"\n",
    "            Applies all the transformations to the text.\n",
    "            \"\"\"\n",
    "            self.original_text = text\n",
    "            self.text = text\n",
    "\n",
    "            # Step 0: preprocessing\n",
    "            self.clean_text()\n",
    "\n",
    "            # Step 1: direct replacements\n",
    "            self.apply_direct_replacements()\n",
    "            self.replace_roman_numerals()\n",
    "\n",
    "            # Step 2: contextual replacements\n",
    "            self.apply_contextual_conversions()\n",
    "\n",
    "            # Step 3: direct conversions\n",
    "            self.apply_direct_conversions()\n",
    "            self.capitalize_post_h_consonants()\n",
    "\n",
    "            # Step 4: word-end transformations\n",
    "            self.word_end_transformation_wrapper()\n",
    "\n",
    "            # Step 5: word initial transformations\n",
    "            self.apply_initial_transformations()\n",
    "\n",
    "            # Step 6: internal word transformations\n",
    "            self.apply_r_transformations()\n",
    "            self.simplify_consonant_clusters()\n",
    "            self.apply_reductor_collisions()\n",
    "            self.apply_other_simplifications()\n",
    "\n",
    "            # Step 7: weak particles assimilation\n",
    "            self.apply_weak_particles_assimilation()\n",
    "\n",
    "            # Step 8: rotacism\n",
    "            if self.rotacism:\n",
    "                self.apply_rotacism()\n",
    "\n",
    "            # Step 9: space-separated 'r' assimilation\n",
    "            self.apply_space_separated_r_assimilation()\n",
    "\n",
    "            return self.text\n",
    "\n",
    "# If running locally, import the class        \n",
    "else:\n",
    "    from src.convert_to_andalusian_spanish import AndalusianConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a formatting function\n",
    "def format_prompt(raw_text):\n",
    "    prompt = f\"### Preƨunʌa: {raw_text}\\n ### Γeьpueьʌa: \"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "def run_inference(prompt: str,\n",
    "                  n_max_tokens: int = 1250,\n",
    "                  convert_text: bool = True):\n",
    "    \"\"\"\n",
    "    Runs inference on MistrAND-7B-v1 model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Andalusian Spanish text use as input for the model.\n",
    "        n_max_tokens (int): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text by the model.\n",
    "    \"\"\"\n",
    "    # Convert text to Andalusian Spanish\n",
    "    if convert_text:\n",
    "        conversor = AndalusianConverter()\n",
    "        converted_text = conversor.convert(prompt)\n",
    "        print(\"Converted text: \")\n",
    "        print(converted_text)\n",
    "        print()\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt = format_prompt(prompt)\n",
    "    model_input = eval_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    ft_model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = eval_tokenizer.decode(ft_model.generate(**model_input, \n",
    "                                                         max_new_tokens = n_max_tokens, \n",
    "                                                         repetition_penalty=1.15)[0], \n",
    "                                        skip_special_tokens=True)\n",
    "\n",
    "    # Format result\n",
    "    try:\n",
    "        formatted_result = result.split('### Γeьpueьʌa: ')[-1]\n",
    "    except IndexError:\n",
    "        raise ValueError(\"Error generating text. Please try again woth another prompt.\")\n",
    "\n",
    "    return formatted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "prompt = \"¡Hola! Quiero ayudar a conservar todos los idiomas del mundo. ¿Me ayudas?\"\n",
    "output = run_inference(prompt, convert_text = True)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
